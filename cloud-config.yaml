#cloud-config
# This is a cloud-init configuration for setting up RKE2 with Cilium and other necessary configurations.
hostname: bioskop-master-control-node
fqdn: bioskop-master-control-node.mammoth-skate.ts.net
manage_resolv_conf: true
resolv_conf:
  searchdomains:
  - mammoth-skate.ts.net

write_files:


  # RKE2 profile with robust PATH and env setup
  - path: /etc/rancher/rke2/profile
    permissions: "0644"
    content: |
      #!/usr/bin/env bash 

      set -a

      : load the container environment variables
      source <( cat /proc/1/environ | tr '\0' '\n' | grep -E '^(CLUSTER_|TSID|TSKEY)' )

      : set the environment variables for RKE2
      ARCH=$( dpkg --print-architecture )
      KUBECONFIG=/etc/rancher/rke2/rke2.yaml
      CONTAINERD_CONFIG_FILE="/var/lib/rancher/rke2/agent/etc/containerd/config.toml"
      CONTAINERD_ADDRESS=/run/k3s/containerd/containerd.sock
      CONTAINERD_NAMESPACE=k8s.io
      CRI_CONFIG_FILE=/var/lib/rancher/rke2/agent/etc/crictl.yaml
      INSTALL_RKE2_TYPE=${INSTALL_RKE2_TYPE:-server}

      : set the PATH for RKE2 and Cilium
      PATH="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
      PATH="$PATH:/var/lib/rancher/rke2/bin"
      PATH="$PATH:/var/lib/rancher/.flox/run/aarch64-linux.rancher.run/bin/"

      set +a

      : Only eval direnv if interactive shell and direnv is present
      if [[ $- == *i* ]] && type -P direnv >/dev/null 2>&1; then
        eval "$(direnv hook bash)"
      fi
 
  # Kubelet cgroup driver override
  - path: /var/lib/rancher/rke2/agent/etc/kubelet.conf.d/01-cgroup-override.conf
    content: |
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      cgroupDriver: systemd

  # RKE2 and Cilium configuration (source profile)
  - path: /usr/local/sbin/rke2-pre-start.sh
    permissions: "0755"
    content: |
      #!/usr/bin/env -S bash -exu -o pipefail

      : Load the RKE2 profile 
      source /etc/rancher/rke2/profile

      : Patch the cilium config with cluster environment variables
      yq --inplace --from-file=<( cat <<EoE | tee /tmp/kube-system.yq
      ( select( .kind == "CiliumBGPAdvertisement" ) | .spec ) |=
        ( .advertisements[0].advertisementType = "PodCIDR" ) |
      ( select( .kind == "CiliumBGPNodeConfigOverride" ) | .spec ) |=
        ( .bgpInstances[0].localPort = 179 ) |
      ( select( .kind == "CiliumBGPNodeConfigOverride" ) | .spec.bgpInstances[0].peers[] ) |=
        ( .peerAddress = "172.31.${CLUSTER_ID}.2" ) |
      ( select( .kind == "HelmChartConfig") | .spec ) |=
        ( .valuesContent |= ( from_yaml | 
                               .cluster.name = "${CLUSTER_NAME}" |
                               .cluster.id = ${CLUSTER_ID} |
                               to_yaml ) ) |
      ( select( .kind == "CiliumLoadBalancerIPPool" ) | .spec ) |=
        ( .blocks[0] = { "cidr": "172.31.${CLUSTER_ID}.128/25", "min": "129", "max": "254" } ) |
      ( select( .kind == "CiliumBGPClusterConfig" ) | .spec ) |=
        with( .bgpInstances[] | select( .name == "instance-65000" ); 
          with( .peers[] | select( .name == "master"); .peerAddress = "172.31.${CLUSTER_ID}.2" ) |
          with( .peers[] | select( .name == "peer1");  .peerAddress = "172.31.${CLUSTER_ID}.3" ) |
          with( .peers[] | select( .name == "peer2");  .peerAddress = "172.31.${CLUSTER_ID}.4" ) |
          with( .peers[] | select( .name == "peer3");  .peerAddress = "172.31.${CLUSTER_ID}.5" ) )
      EoE
      ) /var/lib/rancher/rke2/server/manifests/rke2-cilium-config.yaml
      : Generate the tailscale operator config
      cat <<EoF | tee /var/lib/rancher/rke2/server/manifests/tailscale-operator.yaml                  
      apiVersion: helm.cattle.io/v1
      kind: HelmChart
      metadata:
        namespace: kube-system
        name: tailscale-operator
      spec:
        repo: https://pkgs.tailscale.com/helmcharts
        chart: tailscale-operator
        version: 1.82.0
        targetNamespace: tailscale-system
        createNamespace: true
        valuesContent: |-
          oauth:
            clientId: "${TSID}"
            clientSecret: "${TSKEY}"
          operatorConfig:
            hostname: "${CLUSTER_NAME}-tailscale-operator"
      ---
      apiVersion: tailscale.com/v1alpha1
      kind: Connector
      metadata:
        name: ts-controlplane-lb-routes
      spec:
        hostname: ${CLUSTER_NAME}-controlplane-lb-routes  # Name visible in Tailscale admin
        subnetRouter:
          advertiseRoutes:
      EoF
      : Patch the tailscale operator config with cluster environment variables
      yq --inplace --from-file=<( cat <<EoE
      ( select( .kind == "Connector" ) | .spec ) |=
        ( .subnetRouter.advertiseRoutes = 
          [ "172.31.${CLUSTER_ID}.0/28", "172.31.${CLUSTER_ID}.128/25" ] )
      EoE
      ) /var/lib/rancher/rke2/server/manifests/tailscale-operator.yaml
      : Configure the cluster CIDR and service CIDR
      mkdir -p /etc/rancher/rke2/config.yaml.d
      touch /etc/rancher/rke2/config.yaml.d/cidr.yaml
      yq --inplace --from-file=<( cat <<EoF
      . += { "cluster-cidr": "10.${CLUSTER_ID}.0.0/17", "service-cidr": "10.${CLUSTER_ID}.128.0/17" }
      EoF
      ) /etc/rancher/rke2/config.yaml.d/cidr.yaml
      : Reconfigure the names resolution and the system in read-write mode
      ln -fs /run/NetworkManager/resolv.conf /etc/resolv.conf
      mount -o remount,rw /proc
      mount -o remount,rw /sys

  # Configure RKE2
  - path: /usr/local/sbin/rke2-post-start.sh
    permissions: "0755"
    content: |
      #!/usr/bin/env -S bash -exu -o pipefail

      : Load RKE2 environment
      source /etc/rancher/rke2/profile

      : Configure system-wide RKE2 environment
      mkdir -p /etc/profile.d
      ln -fs /etc/rancher/rke2/profile /etc/profile.d/rke2-env.sh

      : Get current IP address
      source <(ip --json addr show eth0 |
               yq -p json -o shell '.[0].addr_info.[] | select(.family == "inet") | { "inet": .local }')

      : Create working copy of kubeconfig
      KUBECONFIG="/.kubeconfig.d/rke2-${CLUSTER_NAME}.yaml"

      mkdir -p $( dirname "$KUBECONFIG" )
      cp /etc/rancher/rke2/rke2.yaml "$KUBECONFIG"
      chmod 600 "$KUBECONFIG"

      : Apply modifications to working copy
      yq --inplace --from-file=<(cat <<EoE
      .clusters[0].cluster.name = "${CLUSTER_NAME}" |
      .clusters[0].cluster.server = "https://${inet}:6443" |
      .clusters[0].name = "${CLUSTER_NAME}" |
      .contexts[0].context.cluster = "${CLUSTER_NAME}" |
      .contexts[0].context.namespace = "kube-system" |
      .contexts[0].context.user = "${CLUSTER_NAME}" |
      .contexts[0].name = "${CLUSTER_NAME}" |
      .users[0].name = "${CLUSTER_NAME}" |
      .current-context = "${CLUSTER_NAME}"
      EoE
      ) "$KUBECONFIG"

  # RKE2 installation script from https://get.rke2.io
  - path: /usr/local/sbin/rke2-install.sh
    permissions: "0755"
    content: |
      #!/usr/bin/env -S bash -exu -o pipefail

      : Install the RKE2 server binaries
      curl -sfL https://get.rke2.io | 
        env DEBUG=1 sh -

      source /etc/rancher/rke2/profile

      : Patch containerd to use systemd cgroup driver after RKE2 install
      : This ensures compatibility with systemd cgroup v2 hosts
      if [ -f "$CONTAINERD_CONFIG_FILE" ]; then
        sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' "$CONTAINERD_CONFIG_FILE"
      fi

      : Enable and start the RKE2 server service
      systemctl daemon-reload
      systemctl enable rke2-remount-shared
      systemctl enable rke2-server

  # RKE2 configuration
  - path: /etc/rancher/rke2/config.yaml.d/tls-san.yaml
    content: |
      tls-san:
        - localhost
        - master-control-node
        - peer-control-node1
        - peer-control-node2
  - path: /etc/rancher/rke2/config.yaml.d/core.yaml
    content: |
      write-kubeconfig-mode: "0640"
      etcd-expose-metrics: true
      cni:
        - cilium
      ingress-controller: traefik
  - path: /etc/rancher/rke2/config.yaml.d/disable.yaml
    content: |
      # Disable the default snapshot controller (replaced with openebs-zfs)
      disable:
        - rke2-snapshot-controller
        - rke2-snapshot-controller-crd
        - rke2-snapshot-validation-webhook
        - rke2-ingress-nginx
  - path: /var/lib/rancher/rke2/agent/etc/kubelet.conf.d/00-disable-gc.conf
    content: |
      kubelet-arg: 
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      imageGCHighThresholdPercent: 100
      imageGCLowThresholdPercent: 99
  # Traefik Ingress Controller
  - path: /var/lib/rancher/rke2/server/manifests/rke2-traefik-config.yaml
    content: |
      apiVersion: helm.cattle.io/v1
      kind: HelmChartConfig
      metadata:
        name: rke2-traefik
        namespace: kube-system
      spec:
        valuesContent: |-
          additionalArguments:
            - "--api.insecure=true"
          ports:
            web:
              expose: true
            websecure:
              expose: true
  # Cilium
  - path: /var/lib/rancher/rke2/server/manifests/rke2-cilium-config.yaml
    content: |
      apiVersion: helm.cattle.io/v1
      kind: HelmChartConfig
      metadata:
        name: cilium
        namespace: kube-system
      spec:
        valuesContent: |-
          bgpControlPlane:
            enabled: true
          cluster:
            name: "default"
            id: 1
          clustermesh:
            useAPIServer: true
            apiserver:
              service:
                type: LoadBalancer
                loadBalancerClass: io.cilium/bgp-control-plane
          envoy:
            enabled: true
          gatewayAPI:
            enabled: false
          ingressController:
            default: true
            enabled: true
            loadBalancerMode: dedicated
          hubble:
            enabled: true
            relay:
              enabled: true
            ui:
              enabled: true
          kubeProxyReplacement: true
          socketLB:
            hostNamespaceOnly: true
          operator:
            hostNetwork: true
      ---
        apiVersion: "cilium.io/v2alpha1"
        kind: CiliumLoadBalancerIPPool
        metadata:
          name: "pool"
        spec:
          blocks:
            -  cidr: "172.31.255.128/25"
      ---
      apiVersion: "cilium.io/v2alpha1"
      kind: CiliumL2AnnouncementPolicy
      metadata:
        name: l2policy
      spec:
        loadBalancerIPs: true
        interfaces:
          - eth0
      ---
      apiVersion: "cilium.io/v2alpha1"
      kind: CiliumBGPClusterConfig
      metadata:
        name: cilium-bgp
      spec:
        bgpInstances:
          - name: "instance-65000"
            localASN: 65000
            peers:
              - name: "master"
                peerASN: 65000
                peerAddress: "master-control-node"
                peerConfigRef:
                  name: "peer-config-generic"
              - name: "peer1"
                peerASN: 65000
                peerAddress: "peer1-control-node"
                peerConfigRef:
                  name: "peer-config-generic"
              - name: "peer2"
                peerASN: 65000
                peerAddress: "peer2-control-node"
                peerConfigRef:
                  name: "peer-config-generic"
              - name: "peer3"
                peerASN: 65000
                peerAddress: "peer3-control-node"
                peerConfigRef:
                  name: "peer-config-generic"
      ---
      apiVersion: cilium.io/v2alpha1
      kind: CiliumBGPNodeConfigOverride
      metadata:
        name: bgpv2-cplane-dev-multi-homing-worker
      spec:
        bgpInstances:
          - name: "instance-65000"
            localPort: 179
      ---
      apiVersion: "cilium.io/v2alpha1"
      kind: CiliumBGPPeerConfig
      metadata:
        name: peer-config-generic
      spec:
        families:
          - afi: ipv4
            safi: unicast
            advertisements:
              matchLabels:
                advertise: "generic"
      ---
      apiVersion: "cilium.io/v2alpha1"
      kind: CiliumBGPAdvertisement
      metadata:
        name: services
        labels:
          advertise: generic
      spec:
        advertisements:
          - advertisementType: "PodCIDR"
          - advertisementType: "Service"
            service:
              addresses:
                - LoadBalancerIP
            selector: # select all services
              matchExpressions:
                - key: "somekey"
                  operator: In
                  values:
                    - "never-used-value"

  # Envoy Gateway
  - path: /var/lib/rancher/rke2/server/manifests/envoy-gateway.yaml
    content: |
      apiVersion: helm.cattle.io/v1
      kind: HelmChart
      metadata:
        name: envoy-gateway
        namespace: envoy-gateway-system
      spec:
        repo: https://charts.envoyproxy.io
        chart: gateway-helm
        version: 0.5.0
        targetNamespace: envoy-gateway-system
        createNamespace: true
        ---                                                                                                                                                                
        apiVersion: gateway.networking.k8s.io/v1                                                                                                                             
        kind: GatewayClass                                                                                                                                                   
        metadata:                                                                                                                                                            
          name: envoy                                                                                                                                                        
        spec:                                                                                                                                                                
          controllerName: gateway.envoyproxy.io/gatewayclass-controller

  # OpenEBS ZFS PVC
  - path: /var/lib/rancher/rke2/server/manifests/openebs-zfs.yaml
    content: |
      apiVersion: v1
      kind: Namespace
      metadata:
        name: openebs
      ---
      apiVersion: helm.cattle.io/v1
      kind: HelmChart
      metadata:
        name: openebs-zfs
        namespace: openebs
      spec:
        chart: zfs-localpv
        repo: https://openebs.github.io/zfs-localpv
        version: 2.8.0
        targetNamespace: openebs
        createNamespace: true
        valuesContent: |-
          zfsNode:
            kubeletDir: /var/lib/rancher/rke2/agent
          # Optionally set nodeSelector/tolerations if needed
      ---
      apiVersion: storage.k8s.io/v1
      kind: StorageClass
      metadata:
        name: openebs-zfs
      provisioner: zfs.csi.openebs.io
      parameters:
        poolname: tank
        fstype: zfs
      reclaimPolicy: Delete
      volumeBindingMode: WaitForFirstConsumer
      allowVolumeExpansion: true

  # tailscale operator
  - path: /var/lib/rancher/rke2/server/manifests/tailscale-operator.yaml
    content: |
      apiVersion: helm.cattle.io/v1
      kind: HelmChart
      metadata:
        namespace: kube-system
        name: tailscale-operator
      spec:
        repo: https://pkgs.tailscale.com/helmcharts
        chart: tailscale-operator
        version: 1.82.0
        targetNamespace: tailscale-system
        createNamespace: true
        valuesContent: |-
          oauth:
            clientId: "${TSID}"
            clientSecret: "${TSKEY}"
          operatorConfig:
            hostname: "${CLUSTER_NAME}-tailscale-operator"
            debug: true
      ---
      apiVersion: tailscale.com/v1alpha1
      kind: Connector
      metadata:
        name: ts-controlplane-lb-routes
      spec:
        hostname: ${CLUSTER_NAME}-controlplane-lb-routes  # Name visible in Tailscale admin
        subnetRouter:
          advertiseRoutes:

  # Systemd service files
  - path: /etc/systemd/system/rke2-remount-shared.service
    content: |
      [Unit]
      Description=Remount RKE2 required volumes as shared
      Before=rke2-server.service
      DefaultDependencies=no
      [Service]
      Type=oneshot
      ExecStart=/usr/local/sbin/rke2-remount-shared.sh
      RemainAfterExit=true
      [Install]
      WantedBy=multi-user.target

  - path: /etc/systemd/system/rke2-install.service
    content: |
      [Unit]
      Description=Run RKE2 Installation Script
      After=network.target
      ConditionPathExists=/usr/local/sbin/rke2-install.sh
      ConditionPathExists=!/etc/systemd/system/rke2-server.service

      [Install]
      WantedBy=multi-user.target
      RequiredBy=multi-user.target

      [Service]
      Type=oneshot
      ExecStartPre=/usr/local/sbin/rke2-install-pre.sh
      ExecStart=/usr/bin/env -S bash -c 'rke2-install.sh && systemctl disable rke2-install.service'
      RemainAfterExit=true

  - path: /etc/systemd/system/rke2-server.service.d/override.conf
    content: |
      [Service]
      Restart=always
      RestartSec=10s

      [Install]
      WantedBy=multi-user.target

  - path: /usr/local/sbin/rke2-install-pre.sh
    permissions: "0755"
    content: |
      #!/usr/bin/env -S bash -exu -o pipefail

      : Initialize the flox environment
      [[ ! -d /var/lib/rancher/.flox ]] && 
        flox init --dir=/var/lib/rancher

      : install tools in the flox environment
      flox install --dir=/var/lib/rancher direnv
      flox install --dir=/var/lib/rancher yq-go
      flox install --dir=/var/lib/rancher kubernetes-helm
      flox install --dir=/var/lib/rancher cilium-cli
      flox install --dir=/var/lib/rancher ceph-client

      : allow direnv to load flox environment variables
      flox install --dir=/var/lib/rancher direnv

      mkdir -p "~root/.config/direnv/lib"
      curl -o "~root/.config/direnv/lib/flox-direnv.sh" "https://raw.githubusercontent.com/flox/flox-direnv/v1.1.0/direnv.rc"

  - path: /etc/systemd/system/rke2-server.service.d/start.conf
    content: |
      [Unit]
      Requires=rke2-remount-shared.service
      Wants=rke2-remount-shared.service
      After=rke2-remount-shared.service

      [Service]
      ExecStartPre=/usr/local/sbin/rke2-pre-start.sh
      ExecStartPost=/usr/local/sbin/rke2-post-start.sh

  - path: /etc/systemd/system/rke2-agent.service.d/start.conf
    content: |
      [Service]
      ExecStartPre=/usr/local/sbin/rke2-pre-start.sh
      ExecStartPost=/usr/local/sbin/rke2-post-start.sh

  # Scripts (preserve executable bit)
  - path: /usr/local/sbin/rke2-remount-shared.sh
    permissions: "0755"
    content: |
      #!/usr/bin/env -S bash -exu -o pipefail
      mount --make-shared /
      mount --make-shared -t bpf bpf /sys/fs/bpf
      mount --make-shared /run

  - path: /usr/local/sbin/iscsiadm
    permissions: "0755"
    content: |
      #!/usr/bin/env -S bash -exu -o pipefail
      exec chroot /host /usr/bin/env -i PATH="/sbin:/bin:/usr/bin" iscsiadm "${@:1}"

  - path: /usr/local/sbin/rke2-pre-start.sh
    permissions: "0755"
    content: |
      #!/usr/bin/env -S bash -exu -o pipefail

      : Load the RKE2 profile 
      source /etc/rancher/rke2/profile

      : Patch the cilium config with cluster environment variables
      yq --inplace --from-file=<( cat <<EoE | tee /tmp/kube-system.yq
      ( select( .kind == "CiliumBGPAdvertisement" ) | .spec ) |=
        ( .advertisements[0].advertisementType = "PodCIDR" ) |
      ( select( .kind == "CiliumBGPNodeConfigOverride" ) | .spec ) |=
        ( .bgpInstances[0].localPort = 179 ) |
      ( select( .kind == "CiliumBGPNodeConfigOverride" ) | .spec.bgpInstances[0].peers[] ) |=
        ( .peerAddress = "172.31.${CLUSTER_ID}.2" ) |
      ( select( .kind == "HelmChartConfig") | .spec ) |=
        ( .valuesContent |= ( from_yaml | 
                               .cluster.name = "${CLUSTER_NAME}" |
                               .cluster.id = ${CLUSTER_ID} |
                               to_yaml ) ) |
      ( select( .kind == "CiliumLoadBalancerIPPool" ) | .spec ) |=
        ( .blocks[0] = { "cidr": "172.31.${CLUSTER_ID}.128/25", "min": "129", "max": "254" } ) |
      ( select( .kind == "CiliumBGPClusterConfig" ) | .spec ) |=
        with( .bgpInstances[] | select( .name == "instance-65000" ); 
          with( .peers[] | select( .name == "master"); .peerAddress = "172.31.${CLUSTER_ID}.2" ) |
          with( .peers[] | select( .name == "peer1");  .peerAddress = "172.31.${CLUSTER_ID}.3" ) |
          with( .peers[] | select( .name == "peer2");  .peerAddress = "172.31.${CLUSTER_ID}.4" ) |
          with( .peers[] | select( .name == "peer3");  .peerAddress = "172.31.${CLUSTER_ID}.5" ) )
      EoE
      ) /var/lib/rancher/rke2/server/manifests/rke2-cilium-config.yaml
      : Generate the tailscale operator config
      cat <<EoF | tee /var/lib/rancher/rke2/server/manifests/tailscale-operator.yaml                  
      apiVersion: helm.cattle.io/v1
      kind: HelmChart
      metadata:
        namespace: kube-system
        name: tailscale-operator
      spec:
        repo: https://pkgs.tailscale.com/helmcharts
        chart: tailscale-operator
        version: 1.82.0
        targetNamespace: tailscale-system
        createNamespace: true
        valuesContent: |-
          oauth:
            clientId: "${TSID}"
            clientSecret: "${TSKEY}"
          operatorConfig:
            hostname: "${CLUSTER_NAME}-tailscale-operator"
      ---
      apiVersion: tailscale.com/v1alpha1
      kind: Connector
      metadata:
        name: ts-controlplane-lb-routes
      spec:
        hostname: ${CLUSTER_NAME}-controlplane-lb-routes  # Name visible in Tailscale admin
        subnetRouter:
          advertiseRoutes:
      EoF
      : Patch the tailscale operator config with cluster environment variables
      yq --inplace --from-file=<( cat <<EoE
      ( select( .kind == "Connector" ) | .spec ) |=
        ( .subnetRouter.advertiseRoutes = 
          [ "172.31.${CLUSTER_ID}.0/28", "172.31.${CLUSTER_ID}.128/25" ] )
      EoE
      ) /var/lib/rancher/rke2/server/manifests/tailscale-operator.yaml
      : Configure the cluster CIDR and service CIDR
      mkdir -p /etc/rancher/rke2/config.yaml.d
      touch /etc/rancher/rke2/config.yaml.d/cidr.yaml
      yq --inplace --from-file=<( cat <<EoF
      . += { "cluster-cidr": "10.${CLUSTER_ID}.0.0/17", "service-cidr": "10.${CLUSTER_ID}.128.0/17" }
      EoF
      ) /etc/rancher/rke2/config.yaml.d/cidr.yaml
      : Reconfigure the names resolution and the system in read-write mode
      ln -fs /run/NetworkManager/resolv.conf /etc/resolv.conf
      mount -o remount,rw /proc
      mount -o remount,rw /sys

  - path: /usr/local/sbin/rke2-post-start.sh
    permissions: "0755"
    content: |
      #!/usr/bin/env -S bash -exu -o pipefail

      : Load RKE2 environment
      source /etc/rancher/rke2/profile

      : Configure system-wide RKE2 environment
      cat <<'EoRC' | tee -a /etc/bash.profile >/dev/null
      : RKE2 server environment variables
      source /etc/rancher/rke2/profile
      EoRC

      : Get current IP address
      source <(ip --json addr show eth0 |
               yq -p json -o shell '.[0].addr_info.[] | select(.family == "inet") | { "inet": .local }')

      : Create working copy of kubeconfig
      KUBECONFIG="/.kubeconfig.d/rke2-${CLUSTER_NAME}.yaml"

      mkdir -p $( dirname "$KUBECONFIG" )
      cp /etc/rancher/rke2/rke2.yaml "$KUBECONFIG"
      chmod 600 "$KUBECONFIG"

      : Apply modifications to working copy
      yq --inplace --from-file=<(cat <<EoE
      .clusters[0].cluster.name = "${CLUSTER_NAME}" |
      .clusters[0].cluster.server = "https://${inet}:6443" |
      .clusters[0].name = "${CLUSTER_NAME}" |
      .contexts[0].context.cluster = "${CLUSTER_NAME}" |
      .contexts[0].context.namespace = "kube-system" |
      .contexts[0].context.user = "${CLUSTER_NAME}" |
      .contexts[0].name = "${CLUSTER_NAME}" |
      .users[0].name = "${CLUSTER_NAME}" |
      .current-context = "${CLUSTER_NAME}"
      EoE
      ) "$KUBECONFIG"

  - path: /usr/local/sbin/rke2-vip-install.sh
    permissions: "0755"
    content: |
      #!/usr/bin/env -S bash -exu -o pipefail
      : Install kube-vip on master control node
      kubectl apply -f https://kube-vip.io/manifests/rbac.yaml
      : Load kube-vip image
      ctr images pull ghcr.io/kube-vip/kube-vip:latest
      : Generate kube-vip daemonset manifest
      ctr -n k8s.io run --rm --net-host ghcr.io/kube-vip/kube-vip:latest \
        vip /kube-vip manifest daemonset \
          --arp --interface eth0 --address master-control-node --controlplane  --leaderElection \
          --taint --services --inCluster | \
          tee /var/lib/rancher/rke2/server/manifests/kube-vip.yaml

  - path: /usr/local/sbin/rke2-activate
    permissions: "0755"
    content: |
      #!/usr/bin/env -S bash -exu -o pipefail
      : Install RKE2 profile
      cat <<EoF | tee -a /etc/zsh/zshrc | tee -a /etc/bash.bashrc
      source /etc/rancher/rke2/profile
      EoF
      : Configure system-wide DNS
      ln -fs /run/systemd/resolve/resolv.conf /etc/resolv.conf
      : Configure systemd units
      systemctl daemon-reload
      systemctl enable rke2-install
      systemctl start rke2-install
      systemctl start rke2-server

# Enable and start systemd units
runcmd:
  - /usr/local/sbin/rke2-activate

