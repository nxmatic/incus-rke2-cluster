= RKE2 Cluster Network Architecture
:toc: left
:toclevels: 3
:sectnums:
:source-highlighter: rouge

== Overview

This document describes the hierarchical network architecture for the RKE2 cluster deployment using Incus containers.

== Network Hierarchy

The network allocation follows a strict hierarchical structure designed for scalability and isolation:

```
Host Supernet: 10.80.0.0/18 (Class B subnet)
├── Cluster 0: 10.80.0.0/21   (8192 addresses)
├── Cluster 1: 10.80.8.0/21   (8192 addresses) ← bioskop
├── Cluster 2: 10.80.16.0/21  (8192 addresses) ← alcide
├── Cluster 3: 10.80.24.0/21  (8192 addresses)
├── Cluster 4: 10.80.32.0/21  (8192 addresses)
├── Cluster 5: 10.80.40.0/21  (8192 addresses)
├── Cluster 6: 10.80.48.0/21  (8192 addresses)
└── Cluster 7: 10.80.56.0/21  (8192 addresses)
```

== Cluster Network Breakdown (Example: Cluster 1 - bioskop)

Each cluster (`/21`) is subdivided into:

=== Node Networks (/23 each)
```
├── Node 0 (master):  10.80.8.0/23   (512 addresses)
├── Node 1 (peer1):   10.80.10.0/23  (512 addresses)  
├── Node 2 (peer2):   10.80.12.0/23  (512 addresses)
├── Node 3 (peer3):   10.80.14.0/23  (512 addresses)
└── VIP Network:      10.80.15.0/24  (256 addresses)
```

=== Special Purpose Networks

==== LoadBalancer Pool
```
Network: 10.80.8.64/26 (within Node 0 network)
Range:   10.80.8.64 - 10.80.8.127 (64 addresses)
Gateway: 10.80.8.129
Usage:   Cilium LoadBalancer IP Pool
```

==== VIP Network
```
Network:  10.80.15.0/24
Gateway:  10.80.15.1
Usage:    Control plane VIP and inter-node communication
Addresses:
├── 10.80.15.1   - VIP Gateway
├── 10.80.15.10  - master VIP interface
├── 10.80.15.11  - peer1 VIP interface  
├── 10.80.15.12  - peer2 VIP interface
└── 10.80.15.250 - Kubernetes API VIP (managed by kube-vip)
```

== Node Network Configuration

Each node uses three network interfaces:

=== Interface Layout
[cols="1,2,3,2"]
|===
|Interface |Network |Purpose |Gateway

|`lan0`
|192.168.1.0/24 (DHCP)
|**Primary internet access** + local network
|192.168.1.254 (home router)

|`wan0` 
|10.80.16.0/21 (cluster network)
|**Cluster node communication only**
|None (cluster-internal, no gateway)

|`vip0`
|10.80.15.0/24 (VIP network)
|Control plane VIP management
|CLUSTER_VIP_GATEWAY
|===

== Current Network Architecture (November 2025)

=== Network Types and Provisioning

The cluster uses three distinct network types with different provisioning methods:

==== 1. Incus Bridge Network (`wan`)
[cols="1,3"]
|===
|**Type** |Managed Incus bridge
|**Provider** |Incus network subsystem
|**DHCP Server** |dnsmasq (managed by Incus)
|**Network** |10.80.16.0/21 (cluster 2 - alcide)
|**Gateway** |None (cluster-internal only, no routing)
|**Static Leases** |MAC-based (52:54:00:02:00:xx → 10.80.16.1x)
|**NAT** |Disabled (cluster-internal traffic only)
|**IPv6** |fd42:21ea:8a29:f3c9::/64 (no NAT)
|**DNS Mode** |none (no DNS server)
|**Bridge Driver** |native (kernel bridge)
|**Purpose** |**Inter-node cluster communication**
|===

Configuration in Incus:
```yaml
networks:
  - name: wan
    type: bridge
    config:
      ipv4.address: 10.80.16.1/21
      ipv4.dhcp: "true"
      ipv4.dhcp.ranges: 10.80.16.2-10.80.16.9,10.80.16.31-10.80.23.254
      ipv4.nat: "false"
      ipv4.routing: "false"
      ipv6.address: fd42:21ea:8a29:f3c9::1/64
      ipv6.nat: "true"
      dns.mode: none
      bridge.driver: native
      raw.dnsmasq: |
        dhcp-host=52:54:00:02:00:00,10.80.16.10,master
        dhcp-host=52:54:00:02:00:01,10.80.16.11,peer1
        # ... etc
```

Interface attachment (in Incus profile):
```yaml
devices:
  wan0:
    name: wan0
    network: wan
    type: nic
```

==== 2. macvlan on Lima VM Interface (`lan0`)
[cols="1,3"]
|===
|**Type** |macvlan (bridge mode)
|**Provider** |Incus device (parent: vmlan0 on Lima VM)
|**DHCP Server** |Home network router (192.168.1.254)
|**Network** |192.168.1.0/24
|**Gateway** |192.168.1.254 (home router)
|**IP Assignment** |Dynamic DHCP from home network
|**NAT** |Home router (reliable, tested)
|**Purpose** |**Primary internet access + local network**
|===

Parent interface on Lima VM:
```
vmlan0: macvlan on Lima shared network (192.168.1.0/24)
```

Interface attachment (in Incus profile):
```yaml
devices:
  lan0:
    name: lan0
    nictype: macvlan
    parent: vmlan0
    type: nic
```

==== 3. Pod Network (Cilium CNI)
[cols="1,3"]
|===
|**Type** |Overlay network (VXLAN)
|**Provider** |Cilium CNI
|**Network** |10.44.0.0/16 (alcide cluster)
|**Per-Node CIDR** |10.44.x.0/24 (x = node index)
|**Routing** |Cilium managed (BGP optional)
|**Purpose** |Pod-to-pod communication
|===

=== Dual-Interface Configuration

The cluster uses a dual-interface setup optimized for reliability:

==== LAN Interface (`lan0`)
[cols="1,3"]
|===
|Type |macvlan on `vmlan0` (Lima VM interface)
|Network |192.168.1.0/24 (home network)
|DHCP |Provided by home router
|Purpose |**Primary internet gateway** (reliable NAT)
|Metric |100 (preferred route)
|===

==== Cluster Network Interface (`wan0`) 
[cols="1,3"]
|===
|Type |Incus bridge network
|Network |10.80.16.0/21 (cluster 2 - alcide)
|DHCP |Provided by Incus (static leases)
|Purpose |**Inter-node communication only**
|Gateway |None (cluster-internal, no routing)
|===

=== Rationale for LAN-Only Internet Access

**Issue**: Lima VM NAT exhibited unreliable TCP connection behavior:
- ICMP (ping) worked correctly
- TCP connections to external services timed out or were extremely slow
- Affected container image pulls from Docker Hub

**Solution**: Use home router (via `lan0`) as **exclusive** internet gateway:
- Home router provides stable, tested NAT
- Lima VM bridge (`wan0`) provides **only** cluster-internal connectivity
- wan0 interface: `use-routes: false` (no default route, no gateway)
- Simplified routing: only one internet path (lan0)

=== Network Rename Plan

The `wan` network name is now misleading since it no longer provides internet access. Future migration plan:

**Current State**:
```
- Network name: wan (Incus bridge)
- Interface name: wan0 (in containers)
- Purpose: Inter-node communication (NOT internet)
```

**Target State**:
```
- Network name: vmnet (VM network - cluster nodes)
- Interface name: vmnet0 (in containers)  
- Purpose: Explicit cluster-internal networking
```

**Migration Plan:
- Full cluster recreation (network rename not supported live)
- Update all references in:
  * `make.d/incus/incus-preseed.yaml` - network definition
  * `make.d/network/network-config.yaml` - netplan configuration
  * `make.d/cloud-config/*.yaml` - cloud-init files
  * Makefiles and documentation
- Coordinate with any external integrations

**Benefits of Rename**:
- Clear semantic naming (vmnet = VM network for nodes)
- Matches existing pattern (`vmlan0`, `vmwan0` on Lima host)
- Eliminates confusion about "WAN" not providing internet

=== Netplan Configuration

Network interfaces inside containers are configured via netplan (systemd-networkd renderer):

==== Configuration File
Location: `/etc/netplan/01-nocloud.yaml`

Generated from: `make.d/network/network-config.yaml`

==== Interface Parameters

**lan0 (macvlan - PRIMARY)**:
```yaml
lan0:
  dhcp4: true
  dhcp4-overrides:
    use-routes: true       # Accept default route
    use-dns: true          # Use home router DNS (192.168.1.254)
    use-domains: false     # Don't override search domains
    use-hostname: false    # Keep configured hostname
    send-hostname: false   # Privacy - don't send to DHCP
    route-metric: 100      # PRIMARY route priority
  dhcp6: false
  optional: false
```

**wan0 (bridge - CLUSTER)**:
```yaml
wan0:
  dhcp4: true
  dhcp4-overrides:
    use-dns: false         # Don't override LAN DNS
    use-routes: false      # Do NOT accept any routes (cluster-internal only)
  optional: false
```

==== Critical Parameters

[cols="2,1,1,3"]
|===
|Parameter |lan0 |wan0 |Effect

|`use-routes`
|true
|false
|Only lan0 accepts routes. wan0 has no gateway (cluster-internal only)

|`use-dns`
|true
|false
|Only lan0 provides DNS servers to `/etc/resolv.conf`

|`use-domains`
|false
|N/A
|Prevent DHCP from changing search domains

|`dhcp-identifier`
|mac
|implicit
|Use MAC address for DHCP client ID (stable across reboots)
|===

==== Runtime Verification

Check actual routes on a running node:
```bash
ip route show default
# Expected output:
# default via 192.168.1.254 dev lan0 proto dhcp src 192.168.1.x metric 100
# (wan0 should NOT appear - no default route)
```

Check DNS configuration:
```bash
resolvectl status
# Should show DNS servers from lan0 (home router)
```

WARNING: If wan0 shows a default route, internet traffic may fail. Ensure `use-routes: false` is set for wan0 in netplan config.

==== Troubleshooting Network Configuration

**Problem: wan0 Has Default Route**

Old netplan configuration may allow wan0 to have a default route:
```bash
# INCORRECT (old config with use-routes: true):
default via 192.168.1.254 dev lan0 proto dhcp src 192.168.1.x metric 100
default via 10.80.16.1 dev wan0 proto dhcp src 10.80.16.x metric 9999   # Should NOT exist
```

Having ANY route via wan0 can cause issues:
- wan0 bridge is cluster-internal only (no internet access)
- Routing conflicts between lan0 and wan0
- Unnecessary fallback path that doesn't work

**Fix: Apply Updated Netplan Configuration**

1. Push updated netplan config to the node:
```bash
ssh lima-nerd-nixos "incus file push make.d/network/network-config.yaml peer1/etc/netplan/01-nocloud.yaml"
```

2. Apply the configuration:
```bash
ssh lima-nerd-nixos "incus exec peer1 -- netplan apply"
```

3. Verify only lan0 has a default route:
```bash
ssh lima-nerd-nixos "incus exec peer1 -- ip route show default"
# Should show ONLY:
# default via 192.168.1.254 dev lan0 proto dhcp src 192.168.1.x metric 100
# (wan0 should NOT appear)
```

4. Test internet connectivity via lan0:
```bash
ssh lima-nerd-nixos "incus exec peer1 -- curl --interface lan0 -I https://registry-1.docker.io"
# Should be fast (~100ms response)
```

**Problem: Multiple IPs on lan0**

After applying new netplan config, a node may temporarily have both old and new DHCP leases:
```bash
# Example from peer3:
inet 192.168.1.166/24 metric 100 brd 192.168.1.255 scope global dynamic lan0
inet 192.168.1.165/24 brd 192.168.1.255 scope global secondary dynamic noprefixroute lan0
```

This is harmless - the old IP will expire according to the DHCP lease time. The primary IP (metric 100) is used for routing.

**Problem: DNS Not Resolving**

If DNS stops working after netplan changes:

1. Check which interface provides DNS:
```bash
resolvectl status
# Should show DNS servers (e.g., 192.168.1.254) from lan0
```

2. If wan0 is providing DNS instead, check netplan config:
```bash
# lan0 should have:
use-dns: true

# wan0 should have:
use-dns: false
use-routes: false  # Critical: no routes from wan0
```

3. Re-apply netplan if needed:
```bash
netplan apply
```

== Routing Strategy

=== Primary Routing
- **Internet access**: via `lan0` interface **only** using home router gateway (192.168.1.254)
- **Cluster communication**: via `wan0` interface within cluster network (10.80.16.0/21) - **no gateway**
- **VIP management**: via `vip0` interface in dedicated VIP network (10.80.15.0/24)

=== Network Isolation
- Each node has its own `/23` subnet for isolation
- VIP network is separate `/24` for control plane communication
- LoadBalancer pool is carved from master node network

== Pod and Service Networks

=== Cluster-Specific CIDRs (Non-overlapping for Cluster Mesh)

[cols="1,2,2"]
|===
|Cluster |Pod CIDR |Service CIDR

|bioskop (1)
|10.42.0.0/16, fd00:10:42::/48
|10.43.0.0/16, fd00:10:43::/108

|alcide (2)  
|10.44.0.0/16, fd00:10:44::/48
|10.45.0.0/16, fd00:10:45::/108
|===

== Network Generation

The network allocation is automatically generated using `ipcalc` in `network.mk`:

=== Generation Flow
1. **Host networks**: Split `10.80.0.0/18` into 8 clusters (`/21` each)
2. **Cluster networks**: Split cluster `/21` into 4 nodes (`/23` each) + VIP (`/24`)
3. **Node networks**: Generate gateway, broadcast, and prefix for each node
4. **LoadBalancer networks**: Carve `/26` subnet from master node network

=== Generated Files
```
.run.d/network/
├── host-networks.env           # Host supernet allocation
├── cluster-1-networks.env     # Cluster 1 node + VIP networks  
└── cluster-1-node-0-networks.env  # Node 0 specific networks
```

== Benefits of This Architecture

1. **Scalability**: Supports up to 8 clusters with 4 nodes each
2. **Isolation**: Each node has dedicated network space  
3. **Flexibility**: Easy to add new clusters or nodes
4. **Automation**: Network allocation generated via `ipcalc`
5. **Clean Separation**: VIP network separate from node networks
6. **Future-Proof**: Non-overlapping Pod CIDRs for Cluster Mesh

== Troubleshooting

=== Common Issues
- **Overlapping networks**: Ensure VIP and node networks don't overlap
- **Wrong gateways**: Use `NODE_HOST_GATEWAY` for internet, `CLUSTER_VIP_GATEWAY` for VIP
- **Routing conflicts**: Check that VIP interface uses dedicated `/24` network

=== Verification Commands
```bash
# Check routing table
ip route show

# Check interface configuration  
ip addr show

# Verify network generation
make files@network
cat .run.d/network/*.env
```

== UML Network Topology Diagram

The following PlantUML diagram illustrates the layered relationships between the Darwin host, the Lima NixOS VM, macvlan parent interfaces, and the Incus master container interfaces across internal (10.80.x) and external (192.168.x / 172.16.x) networks. It also highlights where outbound IPv4 NAT is now applied on the Darwin host via pf (macOS packet filter) rather than inside the Lima NixOS VM. This reflects the macvlan bypass characteristic: container traffic does not traverse the VM's nftables/iptables chains, so NAT must occur at the outer host layer (en0) for private cluster subnets.

[plantuml,network-topology,svg]
----
@startuml
skinparam monochrome true
skinparam shadowing false
skinparam defaultTextAlignment center
skinparam rectangle {
	BorderColor #444
	RoundCorner 8
}
skinparam componentStyle rectangle

rectangle "Darwin Host (alcide)\nmacOS (pf NAT)" as HOST {
	node "Lima VM (nerd-nixos)\nNixOS" as LIMA {
		frame "Host Interfaces" as HOST_IFACES {
			component "enp0s1 (en0)\nUpstream WAN\n192.168.5.15" as ENP
			component "vmlan0\nParent LAN\n192.168.1.3/24" as VMLAN0
			component "vmwan0\nParent WAN\n172.16.105.3/24" as VMWAN0
		}

		frame "Incus Cluster (alcide)" as CLUSTER {
			node "master container" as MASTER {
				interface "lan0\n192.168.1.x/24" as LAN0
				interface "wan0\n10.80.24.x (/21 slice)" as WAN0
				interface "vip0\n10.80.31.x (/24 VIP)" as VIP0
			}
		}
	}
}

' L2 macvlan relationships (container interfaces bound to host parent NICs)
LAN0 --> VMLAN0 : macvlan (LAN access)
WAN0 --> VMWAN0 : macvlan (cluster node subnet)
VIP0 --> VMWAN0 : macvlan (VIP subnet)

"""
NOTE: Previous documentation referenced nftables-based NAT inside the Lima VM.
In the current design private Incus macvlan interfaces bypass the VM kernel
firewall. NAT is performed by macOS pf on the outer host (interface en0/enp0s1).
"""

' NAT / routing path (pf NAT on Darwin host)
VMWAN0 --> ENP : pf NAT (10.80.16.0/21, 10.80.31.0/24)
ENP --> Internet : upstream

' Notes
note right of WAN0
	Internal Node Subnet
	10.80.24.0/21
end note

note right of VIP0
	Control Plane VIP Subnet
	10.80.31.0/24
end note

note bottom of ENP
	Outbound IPv4 NAT (pf) on Darwin host
	Subnets translated:
	10.80.16.0/21
	10.80.31.0/24
end note

@enduml
----

=== Diagram Legend
* macvlan: Container interface directly on parent host NIC (L2 bypass of VM firewall)
* Internal subnets (10.80.16.0/21, 10.80.31.0/24): Require host-level NAT (pf) for outbound IPv4
* NAT boundary: macOS pf on en0/enp0s1 (not inside Lima VM)
* VIP subnet: Control plane virtual IP allocations (e.g. kube-vip)
* Lima VM firewall note: nftables rules inside VM do not see macvlan-origin traffic

=== Next Steps
If retaining internal subnets (Strategy B), ensure Darwin host pf NAT rules (in `/etc/pf.conf` via anchor):
```bash
# /etc/pf.conf excerpt
anchor "rke2.rancher"
load anchor "rke2.rancher" from "/etc/pf.anchors/rke2.rancher"

# /etc/pf.anchors/rke2.rancher
nat on en0 from 10.80.16.0/21 to any -> (en0)
nat on en0 from 10.80.31.0/24 to any -> (en0)
```
Verification:
```bash
sudo pfctl -f /etc/pf.conf
sudo pfctl -a rke2.rancher -sn
sudo pfctl -vs nat | grep 10.80.16
incus exec master -- ping -c1 8.8.8.8
incus exec master -- curl -4 -s https://ifconfig.me
```
Rollback:
```bash
sudo sed -i '' '/rke2.rancher/d' /etc/pf.conf
sudo rm /etc/pf.anchors/rke2.rancher
sudo pfctl -f /etc/pf.conf -F states
```
Note: nftables NAT inside the Lima VM is ineffective for macvlan-attached Incus interfaces due to L2 bypass.