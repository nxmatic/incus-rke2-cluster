= RKE2 Cluster Network Architecture
:toc: left
:toclevels: 3
:sectnums:
:source-highlighter: rouge

== Overview

This document describes the hierarchical network architecture for the RKE2 cluster deployment using Incus containers.

== Network Hierarchy

The network allocation follows a strict hierarchical structure designed for scalability and isolation:

```
Host Supernet: 10.80.0.0/18 (Class B subnet)
├── Cluster 0: 10.80.0.0/21   (8192 addresses)
├── Cluster 1: 10.80.8.0/21   (8192 addresses) ← bioskop
├── Cluster 2: 10.80.16.0/21  (8192 addresses) ← alcide
├── Cluster 3: 10.80.24.0/21  (8192 addresses)
├── Cluster 4: 10.80.32.0/21  (8192 addresses)
├── Cluster 5: 10.80.40.0/21  (8192 addresses)
├── Cluster 6: 10.80.48.0/21  (8192 addresses)
└── Cluster 7: 10.80.56.0/21  (8192 addresses)
```

== Cluster Network Breakdown (Example: Cluster 1 - bioskop)

Each cluster (`/21`) is subdivided into:

=== Node Networks (/23 each)
```
├── Node 0 (master):  10.80.8.0/23   (512 addresses)
├── Node 1 (peer1):   10.80.10.0/23  (512 addresses)  
├── Node 2 (peer2):   10.80.12.0/23  (512 addresses)
├── Node 3 (peer3):   10.80.14.0/23  (512 addresses)
└── VIP Network:      10.80.15.0/24  (256 addresses)
```

=== Special Purpose Networks

==== LoadBalancer Pool
```
Network: 10.80.8.64/26 (within Node 0 network)
Range:   10.80.8.64 - 10.80.8.127 (64 addresses)
Gateway: 10.80.8.129
Usage:   Cilium LoadBalancer IP Pool
```

==== VIP Network
```
Network:  10.80.15.0/24
Gateway:  10.80.15.1
Usage:    Control plane VIP (managed by kube-vip on vmnet0)
Addresses:
├── 10.80.15.1   - VIP Gateway
├── 10.80.15.250 - Kubernetes API VIP (kube-vip)
```

NOTE: The VIP is announced by kube-vip directly on the `vmnet0` interface. No separate VLAN or vip0 interface is created.

== Node Network Configuration

Each node uses two network interfaces:

=== Interface Layout
[cols="1,2,3,2"]
|===
|Interface |Network |Purpose |Gateway

|`lan0`
|192.168.1.0/24 (DHCP)
|**Primary internet access** + local network
|192.168.1.254 (home router)

|`vmnet0` 
|10.80.x.0/21 (cluster network)
|**Cluster node communication only**
|None (cluster-internal, no gateway)
|===

NOTE: VIP (10.80.15.250) is managed by kube-vip directly on the `vmnet0` interface. No separate vip0 interface exists.

== Current Network Architecture (November 2025)

=== Network Types and Provisioning

The cluster uses two distinct network types:

==== 1. Incus Bridge Network (`vmnet`)
[cols="1,3"]
|===
|**Type** |Managed Incus bridge
|**Provider** |Incus network subsystem
|**DHCP Server** |dnsmasq (managed by Incus)
|**Network** |10.80.x.0/21 (cluster-specific)
|**Gateway** |None (cluster-internal only, no routing)
|**Static Leases** |MAC-based (52:54:00:02:00:xx → 10.80.16.1x)
|**NAT** |Disabled (cluster-internal traffic only)
|**IPv6** |fd42:21ea:8a29:f3c9::/64 (no NAT)
|**DNS Mode** |none (no DNS server)
|**Bridge Driver** |native (kernel bridge)
|**Purpose** |**Inter-node cluster communication**
|===

Configuration in Incus:
```yaml
networks:
  - name: vmnet
    type: bridge
    config:
      ipv4.address: 10.80.x.1/21  # x = CLUSTER_ID * 8
      ipv4.dhcp: "true"
      ipv4.dhcp.ranges: 10.80.x.2-10.80.x.9,10.80.x.31-10.80.(x+7).254
      ipv4.nat: "false"
      ipv4.routing: "false"
      ipv6.address: fd42:xxxx:xxxx:xxxx::1/64
      ipv6.nat: "true"
      dns.mode: none
      bridge.driver: native
      raw.dnsmasq: |
        dhcp-host=52:54:00:0C:00:00,10.80.x.10,master
        dhcp-host=52:54:00:0C:00:01,10.80.x.11,peer1
        # ... etc (C = CLUSTER_ID)
```

Interface attachment (in Incus profile):
```yaml
devices:
  vmnet0:
    name: vmnet0
    network: vmnet
    type: nic
```

==== 2. macvlan on Lima VM Interface (`lan0`)
[cols="1,3"]
|===
|**Type** |macvlan (bridge mode)
|**Provider** |Incus device (parent: vmlan0 on Lima VM)
|**DHCP Server** |Home network router (192.168.1.254)
|**Network** |192.168.1.0/24
|**Gateway** |192.168.1.254 (home router)
|**IP Assignment** |Dynamic DHCP from home network
|**NAT** |Home router (reliable, tested)
|**Purpose** |**Primary internet access + local network**
|===

Parent interface on Lima VM:
```
vmlan0: macvlan on Lima shared network (192.168.1.0/24)
```

Interface attachment (in Incus profile):
```yaml
devices:
  lan0:
    name: lan0
    nictype: macvlan
    parent: vmlan0
    type: nic
```

==== 3. Pod Network (Cilium CNI)
[cols="1,3"]
|===
|**Type** |Overlay network (VXLAN)
|**Provider** |Cilium CNI
|**Network** |10.42.0.0/16 (bioskop) or 10.44.0.0/16 (alcide)
|**Per-Node CIDR** |10.4x.y.0/24 (y = node index)
|**Routing** |Cilium managed (BGP optional)
|**Purpose** |Pod-to-pod communication
|===

=== Dual-Interface Configuration

The cluster uses a dual-interface setup optimized for reliability:

==== LAN Interface (`lan0`)
[cols="1,3"]
|===
|Type |macvlan on `vmlan0` (Lima VM interface)
|Network |192.168.1.0/24 (home network)
|DHCP |Provided by home router
|Purpose |**Primary internet gateway** (reliable NAT)
|Metric |100 (preferred route)
|===

==== Cluster Network Interface (`vmnet0`) 
[cols="1,3"]
|===
|Type |Incus bridge network
|Network |10.80.x.0/21 (x = CLUSTER_ID * 8)
|DHCP |Provided by Incus (static leases)
|Purpose |**Inter-node communication only**
|Gateway |None (cluster-internal, no routing)
|===

=== Rationale for LAN-Only Internet Access

**Issue**: Lima VM NAT exhibited unreliable TCP connection behavior:
- ICMP (ping) worked correctly
- TCP connections to external services timed out or were extremely slow
- Affected container image pulls from Docker Hub

**Solution**: Use home router (via `lan0`) as **exclusive** internet gateway:
- Home router provides stable, tested NAT
- Lima VM bridge (`vmnet0`) provides **only** cluster-internal connectivity
- vmnet0 interface: `use-routes: false` (no default route, no gateway)
- Simplified routing: only one internet path (lan0)

=== Netplan Configuration

Network interfaces inside containers are configured via netplan (systemd-networkd renderer):

==== Configuration File
Location: `/etc/netplan/01-nocloud.yaml`

Generated from: `make.d/network/network-config.yaml`

==== Interface Parameters

**lan0 (macvlan - PRIMARY)**:
```yaml
lan0:
  dhcp4: true
  dhcp4-overrides:
    use-routes: true       # Accept default route
    use-dns: true          # Use home router DNS (192.168.1.254)
    use-domains: false     # Don't override search domains
    use-hostname: false    # Keep configured hostname
    send-hostname: false   # Privacy - don't send to DHCP
    route-metric: 100      # PRIMARY route priority
  dhcp6: false
  optional: false
```

**vmnet0 (bridge - CLUSTER)**:
```yaml
vmnet0:
  dhcp4: true
  dhcp4-overrides:
    use-dns: false         # Don't override LAN DNS
    use-routes: false      # Do NOT accept any routes (cluster-internal only)
  optional: false
```

==== Critical Parameters

[cols="2,1,1,3"]
|===
|Parameter |lan0 |vmnet0 |Effect

|`use-routes`
|true
|false
|Only lan0 accepts routes. vmnet0 has no gateway (cluster-internal only)

|`use-dns`
|true
|false
|Only lan0 provides DNS servers to `/etc/resolv.conf`

|`use-domains`
|false
|N/A
|Prevent DHCP from changing search domains

|`dhcp-identifier`
|mac
|implicit
|Use MAC address for DHCP client ID (stable across reboots)
|===

==== Runtime Verification

Check actual routes on a running node:
```bash
ip route show default
# Expected output:
# default via 192.168.1.254 dev lan0 proto dhcp src 192.168.1.x metric 100
# (vmnet0 should NOT appear - no default route)
```

Check DNS configuration:
```bash
resolvectl status
# Should show DNS servers from lan0 (home router)
```

WARNING: If vmnet0 shows a default route, internet traffic may fail. Ensure `use-routes: false` is set for vmnet0 in netplan config.

==== Troubleshooting Network Configuration

**Problem: vmnet0 Has Default Route**

Old netplan configuration may allow vmnet0 to have a default route:
```bash
# INCORRECT (old config with use-routes: true):
default via 192.168.1.254 dev lan0 proto dhcp src 192.168.1.x metric 100
default via 10.80.x.1 dev vmnet0 proto dhcp src 10.80.x.y metric 9999   # Should NOT exist
```

Having ANY route via vmnet0 can cause issues:
- vmnet0 bridge is cluster-internal only (no internet access)
- Routing conflicts between lan0 and vmnet0
- Unnecessary fallback path that doesn't work

**Fix: Apply Updated Netplan Configuration**

1. Push updated netplan config to the node:
```bash
ssh lima-nerd-nixos "incus file push make.d/network/network-config.yaml peer1/etc/netplan/01-nocloud.yaml"
```

2. Apply the configuration:
```bash
ssh lima-nerd-nixos "incus exec peer1 -- netplan apply"
```

3. Verify only lan0 has a default route:
```bash
ssh lima-nerd-nixos "incus exec peer1 -- ip route show default"
# Should show ONLY:
# default via 192.168.1.254 dev lan0 proto dhcp src 192.168.1.x metric 100
# (vmnet0 should NOT appear)
```

4. Test internet connectivity via lan0:
```bash
ssh lima-nerd-nixos "incus exec peer1 -- curl --interface lan0 -I https://registry-1.docker.io"
# Should be fast (~100ms response)
```

**Problem: Multiple IPs on lan0**

After applying new netplan config, a node may temporarily have both old and new DHCP leases:
```bash
# Example from peer3:
inet 192.168.1.166/24 metric 100 brd 192.168.1.255 scope global dynamic lan0
inet 192.168.1.165/24 brd 192.168.1.255 scope global secondary dynamic noprefixroute lan0
```

This is harmless - the old IP will expire according to the DHCP lease time. The primary IP (metric 100) is used for routing.

**Problem: DNS Not Resolving**

If DNS stops working after netplan changes:

1. Check which interface provides DNS:
```bash
resolvectl status
# Should show DNS servers (e.g., 192.168.1.254) from lan0
```

2. If vmnet0 is providing DNS instead, check netplan config:
```bash
# lan0 should have:
use-dns: true

# vmnet0 should have:
use-dns: false
use-routes: false  # Critical: no routes from vmnet0
```

3. Re-apply netplan if needed:
```bash
netplan apply
```

== Routing Strategy

=== Primary Routing
- **Internet access**: via `lan0` interface **only** using home router gateway (192.168.1.254)
- **Cluster communication**: via `vmnet0` interface within cluster network (10.80.x.0/21) - **no gateway**

=== Network Isolation
- Single Incus bridge `vmnet` provides cluster-internal communication
- VIP (10.80.15.250) is managed by kube-vip directly on `vmnet0` interface
- Each node has its own static IP in the cluster network
- LoadBalancer services share the cluster network

== Pod and Service Networks

=== Cluster-Specific CIDRs (Non-overlapping for Cluster Mesh)

[cols="1,2,2"]
|===
|Cluster |Pod CIDR |Service CIDR

|bioskop (1)
|10.42.0.0/16, fd00:10:42::/48
|10.43.0.0/16, fd00:10:43::/108

|alcide (2)  
|10.44.0.0/16, fd00:10:44::/48
|10.45.0.0/16, fd00:10:45::/108
|===

== Network Generation

The network allocation is automatically generated using `ipcalc` in `network.mk`:

=== Generation Flow
1. **Host networks**: Split `10.80.0.0/18` into 8 clusters (`/21` each)
2. **Cluster networks**: Split cluster `/21` into 4 nodes (`/23` each) + VIP (`/24`)
3. **Node networks**: Generate gateway, broadcast, and prefix for each node
4. **LoadBalancer networks**: Carve `/26` subnet from master node network

=== Generated Files
```
.run.d/network/
├── host-networks.env           # Host supernet allocation
├── cluster-1-networks.env     # Cluster 1 node + VIP networks  
└── cluster-1-node-0-networks.env  # Node 0 specific networks
```

== Benefits of This Architecture

1. **Scalability**: Supports up to 8 clusters with 4 nodes each
2. **Isolation**: Each node has dedicated network space  
3. **Flexibility**: Easy to add new clusters or nodes
4. **Automation**: Network allocation generated via `ipcalc`
5. **Clean Separation**: VIP network separate from node networks
6. **Future-Proof**: Non-overlapping Pod CIDRs for Cluster Mesh

== Troubleshooting

=== Common Issues
- **Overlapping networks**: Ensure VIP and node networks don't overlap
- **Wrong gateways**: Use `NODE_HOST_GATEWAY` for internet, `CLUSTER_VIP_GATEWAY` for VIP
- **Routing conflicts**: Check that VIP interface uses dedicated `/24` network

=== Verification Commands
```bash
# Check routing table
ip route show

# Check interface configuration  
ip addr show

# Verify network generation
make files@network
cat .run.d/network/*.env
```

== UML Network Topology Diagram

The following PlantUML diagram illustrates the layered relationships between the Darwin host, the Lima NixOS VM, macvlan parent interfaces, and the Incus master container interfaces across internal (10.80.x) and external (192.168.x / 172.16.x) networks. It also highlights where outbound IPv4 NAT is now applied on the Darwin host via pf (macOS packet filter) rather than inside the Lima NixOS VM. This reflects the macvlan bypass characteristic: container traffic does not traverse the VM's nftables/iptables chains, so NAT must occur at the outer host layer (en0) for private cluster subnets.

[plantuml,network-topology,svg]
----
@startuml
skinparam monochrome true
skinparam shadowing false
skinparam defaultTextAlignment center
skinparam rectangle {
	BorderColor #444
	RoundCorner 8
}
skinparam componentStyle rectangle

rectangle "Darwin Host (alcide)\nmacOS (pf NAT)" as HOST {
	node "Lima VM (nerd-nixos)\nNixOS" as LIMA {
		frame "Host Interfaces" as HOST_IFACES {
			component "enp0s1 (en0)\nUpstream WAN\n192.168.5.15" as ENP
			component "vmlan0\nParent LAN\n192.168.1.3/24" as VMLAN0
			component "vmnet0\nIncus Bridge\n10.80.x.1" as VMNET0
		}

		frame "Incus Cluster (alcide)" as CLUSTER {
			node "master container" as MASTER {
				interface "lan0\n192.168.1.x/24" as LAN0
				interface "vmnet0\n10.80.x.y/21" as CONTAINER_VMNET0
			}
		}
	}
}

' L2 relationships
LAN0 --> VMLAN0 : macvlan (LAN/internet access)
CONTAINER_VMNET0 --> VMNET0 : Incus managed (cluster-internal)

"""
NOTE: vmnet0 is an Incus-managed bridge providing cluster-internal communication.
No NAT is configured for this network - it is purely for node-to-node communication.
Internet access is via lan0 through the home router (192.168.1.254).
"""

' Internet path via home router
VMLAN0 --> ENP : Lima VM routing
ENP --> Internet : via home router

' Notes
note right of CONTAINER_VMNET0
	Cluster Internal Network
	10.80.x.0/21
	No gateway, no internet
	VIP managed by kube-vip
end note

note right of LAN0
	Home LAN / Internet Access
	192.168.1.0/24
	Gateway: 192.168.1.254
end note

note bottom of ENP
	No NAT configuration needed
	Internet via home router on lan0
	vmnet0 is cluster-internal only
end note

@enduml
----

=== Diagram Legend
* macvlan: Container interface directly on parent host NIC (lan0 for internet access)
* Incus bridge (vmnet): Managed by Incus for cluster-internal communication only
* vmnet0 has no gateway and no internet access - purely for node-to-node communication
* VIP (10.80.15.250) is managed by kube-vip directly on vmnet0 interface
* Internet access is exclusively via lan0 through home router (192.168.1.254)

=== Network Summary
Current architecture uses:
* Single Incus bridge `vmnet` for all cluster nodes
* Static DHCP leases via dnsmasq in Incus
* No VLANs, no separate VIP interface
* Internet via lan0/home router only
* VIP managed by kube-vip on vmnet0 interface directly