= RKE2 Cluster Network Architecture
:toc: left
:toclevels: 3
:sectnums:
:source-highlighter: rouge

== Overview

This document describes the hierarchical network architecture for the RKE2 cluster deployment using Incus containers.

== Lima VM Deterministic Network Mask Update (@codebase)

Lima virtual machine networking for each Darwin host now adopts the full cluster `/21` netmask (`255.255.248.0`) matching the documented cluster slice (e.g. `10.80.8.0/21` for bioskop, `10.80.16.0/21` for alcide). Previously only the first `/24` was exposed via vmnet. DHCP leases are still intentionally constrained to the first `/24` (ending at `.224`) for stability and predictable reuse while higher address ranges remain reserved for future static assignments (service networks, overlay experiments, additional Lima instances).

Summary:
* Netmask widened: `/24 -> /21` (255.255.255.0 → 255.255.248.0)
* Gateway unchanged: first host IP of first `/24` (e.g. `10.80.8.1`)
* DHCP range limited: up to `10.80.<base>.224` (can be expanded later to include additional `/24` slices by adjusting `dhcpEnd`)
* Rationale: Avoid later disruptive netmask changes, reserve contiguous space for future segmentation (e.g. app, infra, storage sub‑ranges), and align Lima VM view with cluster planning diagrams.

To revert to the legacy narrow mask, set `lima.networks.netmask = "255.255.255.0"` in host configuration. To expand DHCP to the full `/21`, update `dhcpEnd` to the last desired address (e.g. `10.80.15.254` for bioskop's cluster slice).

== Network Hierarchy

The network allocation follows a strict hierarchical structure designed for scalability and isolation:

```
Host Supernet: 10.80.0.0/18 (Class B subnet)
├── Cluster 0: 10.80.0.0/21   (8192 addresses)
├── Cluster 1: 10.80.8.0/21   (8192 addresses) ← bioskop
├── Cluster 2: 10.80.16.0/21  (8192 addresses) ← alcide
├── Cluster 3: 10.80.24.0/21  (8192 addresses)
├── Cluster 4: 10.80.32.0/21  (8192 addresses)
├── Cluster 5: 10.80.40.0/21  (8192 addresses)
├── Cluster 6: 10.80.48.0/21  (8192 addresses)
└── Cluster 7: 10.80.56.0/21  (8192 addresses)
```

== Cluster Network Breakdown (Example: Cluster 1 - bioskop)

Each cluster (`/21`) is subdivided into:

=== Node Networks (/23 each)
```
├── Node 0 (master):  10.80.8.0/23   (512 addresses)
├── Node 1 (peer1):   10.80.10.0/23  (512 addresses)  
├── Node 2 (peer2):   10.80.12.0/23  (512 addresses)
├── Node 3 (peer3):   10.80.14.0/23  (512 addresses)
└── VIP Network:      10.80.15.0/24  (256 addresses)
```

=== Special Purpose Networks

==== LoadBalancer Pool
```
Network: 10.80.8.64/26 (within Node 0 network)
Range:   10.80.8.64 - 10.80.8.127 (64 addresses)
Gateway: 10.80.8.129
Usage:   Cilium LoadBalancer IP Pool
```

==== VIP Network
```
Network:  10.80.15.0/24
Gateway:  10.80.15.1
Usage:    Control plane VIP and inter-node communication
Addresses:
├── 10.80.15.1   - VIP Gateway
├── 10.80.15.10  - master VIP interface
├── 10.80.15.11  - peer1 VIP interface  
├── 10.80.15.12  - peer2 VIP interface
└── 10.80.15.250 - Kubernetes API VIP (managed by kube-vip)
```

== Node Network Configuration

Each node uses three network interfaces:

=== Interface Layout
[cols="1,2,3,2"]
|===
|Interface |Network |Purpose |Gateway

|`lan0`
|192.168.1.0/24 (DHCP)
|Local network access
|192.168.1.1

|`wan0` 
|Node network (/23)
|Internet access & cluster communication
|NODE_HOST_GATEWAY

|`vip0`
|10.80.15.0/24 (VIP network)
|Control plane VIP management
|CLUSTER_VIP_GATEWAY
|===

=== Master Node Example (10.80.8.0/23)
```
lan0: 192.168.1.84/24 (DHCP) → Internet via 192.168.1.1
wan0: 10.80.8.3/24           → Cluster via 10.80.8.1
vip0: 10.80.15.10/24         → VIP via 10.80.15.1
```

== Routing Strategy

=== Primary Routing
- **Internet access**: via `wan0` interface using `NODE_HOST_GATEWAY`
- **Cluster communication**: via `wan0` interface within node network
- **VIP management**: via `vip0` interface in dedicated VIP network

=== Network Isolation
- Each node has its own `/23` subnet for isolation
- VIP network is separate `/24` for control plane communication
- LoadBalancer pool is carved from master node network

== Pod and Service Networks

=== Cluster-Specific CIDRs (Non-overlapping for Cluster Mesh)

[cols="1,2,2"]
|===
|Cluster |Pod CIDR |Service CIDR

|bioskop (1)
|10.42.0.0/16, fd00:10:42::/48
|10.43.0.0/16, fd00:10:43::/108

|alcide (2)  
|10.44.0.0/16, fd00:10:44::/48
|10.45.0.0/16, fd00:10:45::/108
|===

== Network Generation

The network allocation is automatically generated using `ipcalc` in `network.mk`:

=== Generation Flow
1. **Host networks**: Split `10.80.0.0/18` into 8 clusters (`/21` each)
2. **Cluster networks**: Split cluster `/21` into 4 nodes (`/23` each) + VIP (`/24`)
3. **Node networks**: Generate gateway, broadcast, and prefix for each node
4. **LoadBalancer networks**: Carve `/26` subnet from master node network

=== Generated Files
```
.run.d/network/
├── host-networks.env           # Host supernet allocation
├── cluster-1-networks.env     # Cluster 1 node + VIP networks  
└── cluster-1-node-0-networks.env  # Node 0 specific networks
```

== Variable Naming Convention

=== Modern Network Variables (from network.mk)
```bash
# Host level
HOST_SUPERNET_CIDR=10.80.0.0/18
HOST_CLUSTER_1_NETWORK=10.80.8.0/21

# Cluster level (legacy examples shown for historical context)
CLUSTER_NETWORK_CIDR=10.80.8.0/21
CLUSTER_VIP_NETWORK=10.80.15.0/24
CLUSTER_VIP_GATEWAY=10.80.15.1
CLUSTER_LOADBALANCERS_NETWORK=10.80.8.64/26

# Canonical exported RKE2 variables (preferred going forward)
CLUSTER_NETWORK_CIDR=10.80.8.0/21
CLUSTER_VIP_NETWORK_CIDR=10.80.15.0/24
CLUSTER_VIP_GATEWAY_IP=10.80.15.1
CLUSTER_LOADBALANCER_NETWORK_CIDR=10.80.8.64/26

# Node level
NODE_NETWORK_CIDR=10.80.8.0/23
NODE_HOST_GATEWAY=10.80.8.1
NODE_HOST_NETWORK=10.80.8.0
```

=== Template Variable Mappings
```makefile
# Legacy → Canonical template variable mappings
CLUSTER_LOADBALANCERS_CIDR := $(CLUSTER_LOADBALANCERS_NETWORK)   # legacy; replaced by CLUSTER_LOADBALANCER_NETWORK_CIDR
CLUSTER_VIRTUAL_CIDR := $(CLUSTER_VIP_NETWORK)                   # legacy; replaced by CLUSTER_VIP_NETWORK_CIDR
CLUSTER_VIP_BRIDGE_CIDR := $(CLUSTER_VIP_NETWORK)                # legacy; replaced by CLUSTER_VIP_NETWORK_CIDR

# Canonical template variables (used in cloud-config templates)
CLUSTER_LOADBALANCER_NETWORK_CIDR := $(CLUSTER_LOADBALANCER_NETWORK_CIDR)
CLUSTER_VIP_NETWORK_CIDR := $(CLUSTER_VIP_NETWORK_CIDR)
CLUSTER_VIP_BRIDGE_CIDR := $(CLUSTER_VIP_NETWORK_CIDR)
```

== Benefits of This Architecture

1. **Scalability**: Supports up to 8 clusters with 4 nodes each
2. **Isolation**: Each node has dedicated network space  
3. **Flexibility**: Easy to add new clusters or nodes
4. **Automation**: Network allocation generated via `ipcalc`
5. **Clean Separation**: VIP network separate from node networks
6. **Future-Proof**: Non-overlapping Pod CIDRs for Cluster Mesh

== Troubleshooting

=== Common Issues
- **Overlapping networks**: Ensure VIP and node networks don't overlap
- **Wrong gateways**: Use `NODE_HOST_GATEWAY` for internet, `CLUSTER_VIP_GATEWAY` for VIP
- **Routing conflicts**: Check that VIP interface uses dedicated `/24` network

=== Verification Commands
```bash
# Check routing table
ip route show

# Check interface configuration  
ip addr show

# Verify network generation
make files@network
cat .run.d/network/*.env
```

== UML Network Topology Diagram

The following PlantUML diagram illustrates the layered relationships between the Darwin host, the Lima NixOS VM, macvlan parent interfaces, and the Incus master container interfaces across internal (10.80.x) and external (192.168.x / 172.16.x) networks. It also highlights where outbound IPv4 NAT is now applied on the Darwin host via pf (macOS packet filter) rather than inside the Lima NixOS VM. This reflects the macvlan bypass characteristic: container traffic does not traverse the VM's nftables/iptables chains, so NAT must occur at the outer host layer (en0) for private cluster subnets.

[plantuml,network-topology,svg]
----
@startuml
skinparam monochrome true
skinparam shadowing false
skinparam defaultTextAlignment center
skinparam rectangle {
	BorderColor #444
	RoundCorner 8
}
skinparam componentStyle rectangle

rectangle "Darwin Host (alcide)\nmacOS (pf NAT)" as HOST {
	node "Lima VM (nerd-nixos)\nNixOS" as LIMA {
		frame "Host Interfaces" as HOST_IFACES {
			component "enp0s1 (en0)\nUpstream WAN\n192.168.5.15" as ENP
			component "vmlan0\nParent LAN\n192.168.1.3/24" as VMLAN0
			component "vmwan0\nParent WAN\n172.16.105.3/24" as VMWAN0
		}

		frame "Incus Cluster (alcide)" as CLUSTER {
			node "master container" as MASTER {
				interface "lan0\n192.168.1.x/24" as LAN0
				interface "wan0\n10.80.24.x (/21 slice)" as WAN0
				interface "vip0\n10.80.31.x (/24 VIP)" as VIP0
			}
		}
	}
}

' L2 macvlan relationships (container interfaces bound to host parent NICs)
LAN0 --> VMLAN0 : macvlan (LAN access)
WAN0 --> VMWAN0 : macvlan (cluster node subnet)
VIP0 --> VMWAN0 : macvlan (VIP subnet)

"""
NOTE: Previous documentation referenced nftables-based NAT inside the Lima VM.
In the current design private Incus macvlan interfaces bypass the VM kernel
firewall. NAT is performed by macOS pf on the outer host (interface en0/enp0s1).
"""

' NAT / routing path (pf NAT on Darwin host)
VMWAN0 --> ENP : pf NAT (10.80.16.0/21, 10.80.31.0/24)
ENP --> Internet : upstream

' Notes
note right of WAN0
	Internal Node Subnet
	10.80.24.0/21
end note

note right of VIP0
	Control Plane VIP Subnet
	10.80.31.0/24
end note

note bottom of ENP
	Outbound IPv4 NAT (pf) on Darwin host
	Subnets translated:
	10.80.16.0/21
	10.80.31.0/24
end note

@enduml
----

=== Diagram Legend
* macvlan: Container interface directly on parent host NIC (L2 bypass of VM firewall)
* Internal subnets (10.80.16.0/21, 10.80.31.0/24): Require host-level NAT (pf) for outbound IPv4
* NAT boundary: macOS pf on en0/enp0s1 (not inside Lima VM)
* VIP subnet: Control plane virtual IP allocations (e.g. kube-vip)
* Lima VM firewall note: nftables rules inside VM do not see macvlan-origin traffic

=== Next Steps
If retaining internal subnets (Strategy B), ensure Darwin host pf NAT rules (in `/etc/pf.conf` via anchor):
```bash
# /etc/pf.conf excerpt
anchor "rke2.rancher"
load anchor "rke2.rancher" from "/etc/pf.anchors/rke2.rancher"

# /etc/pf.anchors/rke2.rancher
nat on en0 from 10.80.16.0/21 to any -> (en0)
nat on en0 from 10.80.31.0/24 to any -> (en0)
```
Verification:
```bash
sudo pfctl -f /etc/pf.conf
sudo pfctl -a rke2.rancher -sn
sudo pfctl -vs nat | grep 10.80.16
incus exec master -- ping -c1 8.8.8.8
incus exec master -- curl -4 -s https://ifconfig.me
```
Rollback:
```bash
sudo sed -i '' '/rke2.rancher/d' /etc/pf.conf
sudo rm /etc/pf.anchors/rke2.rancher
sudo pfctl -f /etc/pf.conf -F states
```
Note: nftables NAT inside the Lima VM is ineffective for macvlan-attached Incus interfaces due to L2 bypass.