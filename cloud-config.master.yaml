#cloud-config
# Master node configuration (first control plane node)
# Includes both master-specific settings AND bootstrap-only components

name: master-control-node

write_files:
  # =============================================================================
  # MASTER NODE SPECIFIC CONFIGURATION
  # =============================================================================

  # ETCD configuration for master node
  - path: /etc/rancher/rke2/config.yaml.d/etcd.yaml
    content: |
      etcd-expose-metrics: true
      node-name: master-control-node
      with-node-id: false

  # =============================================================================
  # BOOTSTRAP-ONLY KUBERNETES MANIFESTS
  # =============================================================================

  # Cilium CNI HelmChartConfig (bootstrap only - master deploys, peers inherit)
  - path: /var/lib/rancher/rke2/server/manifests/rke2-cilium-config.yaml
    content: |
      apiVersion: helm.cattle.io/v1
      kind: HelmChartConfig
      metadata:
        name: rke2-cilium
        namespace: kube-system
      spec:
        valuesContent: |-
          bgpControlPlane:
            enabled: true
          cluster:
            name: "default"
            id: 1
          clustermesh:
            useAPIServer: true
            apiserver:
              service:
                type: LoadBalancer
                loadBalancerClass: io.cilium/bgp-control-plane
          envoy:
            enabled: true
          gatewayAPI:
            enabled: true
          ingressController:
            default: true
            enabled: true
            loadBalancerMode: dedicated
          hubble:
            enabled: true
            relay:
              enabled: true
            ui:
              enabled: true
          kubeProxyReplacement: true
          socketLB:
            hostNamespaceOnly: true
          operator:
            hostNetwork: true
            replicas: 1

  # Cilium cluster-wide resources (bootstrap only - applied once on master)
  - path: /var/lib/rancher/rke2/server/manifests/cilium-cluster-resources.yaml
    content: |
      # Cilium LoadBalancer IP Pools
      apiVersion: "cilium.io/v2alpha1"
      kind: CiliumLoadBalancerIPPool
      metadata:
        name: "load-balancers"
      spec:
        blocks:
          - cidr: "${CLUSTER_LOADBALANCERS_CIDR}"
            min: "129"
            max: "254"
      ---
      apiVersion: "cilium.io/v2alpha1"
      kind: CiliumLoadBalancerIPPool
      metadata:
        name: "virtual-addresses"
      spec:
        blocks:
          - cidr: "${CLUSTER_VIRTUAL_ADDRESSES_CIDR}"
            min: "2"
            max: "254"
      ---
      apiVersion: "cilium.io/v2alpha1"
      kind: CiliumL2AnnouncementPolicy
      metadata:
        name: l2policy
      spec:
        loadBalancerIPs: true
        interfaces:
          - eth0
        nodeSelector:
          matchLabels:
            node-role.kubernetes.io/control-plane: "true"
      ---
      apiVersion: "cilium.io/v2alpha1"
      kind: CiliumBGPClusterConfig
      metadata:
        name: cilium-control-plane
      spec:
        nodeSelector:
          matchLabels:
            node-role.kubernetes.io/control-plane: "true"
        bgpInstances:
          - name: "instance-65000"
            localASN: 65000
            peers:
              - name: "master"
                peerASN: 65000
                peerAddress: "172.31.1.2"
                peerConfigRef:
                  name: "peer-config-generic"
              - name: "peer1"
                peerASN: 65000
                peerAddress: "172.31.2.2"
                peerConfigRef:
                  name: "peer-config-generic"
              - name: "peer2"
                peerASN: 65000
                peerAddress: "172.31.3.2"
                peerConfigRef:
                  name: "peer-config-generic"
      ---
      apiVersion: "cilium.io/v2alpha1"
      kind: CiliumBGPNodeConfigOverride
      metadata:
        name: bgpv2-cplane-dev-multi-homing-worker
      spec:
        bgpInstances:
          - name: "instance-65000"
            localPort: 179
      ---
      apiVersion: "cilium.io/v2alpha1"
      kind: CiliumBGPPeerConfig
      metadata:
        name: peer-config-generic
      spec:
        families:
          - afi: ipv4
            safi: unicast
            advertisements:
              matchLabels:
                advertise: "generic"
      ---
      apiVersion: "cilium.io/v2alpha1"
      kind: CiliumBGPAdvertisement
      metadata:
        name: services
        labels:
          advertise: generic
      spec:
        advertisements:
          - advertisementType: "PodCIDR"
          - advertisementType: "Service"
            service:
              addresses:
                - LoadBalancerIP
              selector:
                matchLabels:
                  # Match all LoadBalancer services
                  advertise-bgp: "true"

  # Control plane load balancer service (bootstrap only)
  - path: /var/lib/rancher/rke2/server/manifests/control-plane-lb.yaml
    content: |
      apiVersion: v1
      kind: Service
      metadata:
        name: control-plane-lb
        namespace: kube-system
        labels:
          advertise-bgp: "true"
        annotations:
          io.cilium/lb-ipam-ips: "172.31.0.2"
      spec:
        type: LoadBalancer
        loadBalancerClass: io.cilium/bgp-control-plane
        ports:
        - name: kube-apiserver
          port: 6443
          protocol: TCP
          targetPort: 6443
        selector:
          component: kube-apiserver
          tier: control-plane

  # =============================================================================
  # MASTER-ONLY SCRIPTS AND SYSTEMD OVERRIDES
  # =============================================================================

  # Master-specific pre-start script (with manifest patching)
  - path: /usr/local/sbin/rke2-pre-start-master
    permissions: "0755"
    content: |
      #!/usr/bin/env -S bash -exu -o pipefail
      
      source <( flox activate --dir /var/lib/rancher/rke2 )

      cilium::patch() {
        local config_file="/var/lib/rancher/rke2/server/manifests/rke2-cilium-config.yaml"
        local resources_file="/var/lib/rancher/rke2/server/manifests/cilium-cluster-resources.yaml"
        
        # Patch the HelmChartConfig (shared across all servers)
        if [[ -r "$config_file" ]]; then
          yq --inplace --from-file=<( cat <<EoE | cut -c 5-
          ( select( .kind == "HelmChartConfig") | .spec ) |=
            ( .valuesContent |= ( from_yaml |
                                   .cluster.name = "${CLUSTER_NAME}" |
                                   .cluster.id = ${CLUSTER_SUBNET} |
                                   to_yaml ) )
      EoE
          ) "$config_file"
        fi
        
        # Patch the cluster resources (master only)
        if [[ -r "$resources_file" ]]; then
          yq --inplace --from-file=<( cat <<EoE | cut -c 5-
          ( select( .kind == "CiliumBGPAdvertisement" ) | .spec ) |=
            ( .advertisements[0].advertisementType = "PodCIDR" ) |
          ( select( .kind == "CiliumBGPNodeConfigOverride" ) | .spec ) |=
            ( .bgpInstances[0].localPort = 179 ) |
          ( select( .kind == "CiliumLoadBalancerIPPool" and .metadata.name == "load-balancers" ) | .spec ) |=
            ( .blocks[0] = { "cidr": "${CLUSTER_LOADBALANCERS_CIDR}", "min": "129", "max": "254" } ) |
          ( select( .kind == "CiliumLoadBalancerIPPool" and .metadata.name == "virtual-addresses" ) | .spec ) |=
            ( .blocks[0] = { "cidr": "${CLUSTER_VIRTUAL_ADDRESSES_CIDR}", "min": "2", "max": "254" } ) |
          ( select( .kind == "CiliumBGPClusterConfig" ) | .spec ) |=
            with( .bgpInstances[] | select( .name == "instance-65000" );
              with( .peers[] | select( .name == "master"); .peerAddress = "172.31.1.2" ) |
              with( .peers[] | select( .name == "peer1");  .peerAddress = "172.31.2.2" ) |
              with( .peers[] | select( .name == "peer2");  .peerAddress = "172.31.3.2" ) )
      EoE
          ) "$resources_file"
        fi
      }

      tailscale::patch() {
        local file="/var/lib/rancher/rke2/server/manifests/tailscale-operator.yaml"
        if [[ ! -r "$file" ]]; then
          echo "Error: Tailscale operator file not found: $file"
          return 1
        fi
        yq --inplace --from-file=<( cat <<EoE | cut -c3-
        ( select( .kind == "HelmChart" ) | .spec ) |=
          ( .valuesContent |= ( from_yaml |
                                 .oauth.clientId = "${TSID}" |
                                 .oauth.clientSecret = "${TSKEY}" |
                                 .operatorConfig.hostname = "${CLUSTER_NAME}-${CLUSTER_NODE_NAME}-tailscale-operator" |
                                 to_yaml ) ) |
        ( select( .kind == "Connector" ) | .spec ) |=
          ( .subnetRouter.advertiseRoutes = ["172.31.1.1/28", "172.31.2.1/28"] |
            .hostname = "${CLUSTER_NAME}-controlplane-lb-routes" )
      EoE
        ) "$file"
      }

      : Create RKE2 folders
      mkdir -p /var/lib/rancher/rke2/agent
      mkdir -p /var/lib/rancher/rke2/server
      
      : Patch the cilium config with cluster environment variables
      cilium::patch
      
      : Patch the tailscale operator with cluster environment variables
      tailscale::patch

  # Master-specific systemd override for pre-start (calls master-specific script)
  - path: /etc/systemd/system/rke2-server.service.d/pre-start-master.conf
    content: |
      [Service]
      ExecStartPre=/bin/sh -xc '/usr/local/sbin/rke2-pre-start-master'

  # Cilium validation and check script (master only)
  - path: /usr/local/sbin/rke2-cilium-check
    permissions: "0755"
    content: |
      #!/usr/bin/env -S bash -xu -o pipefail
      
      source <( flox activate --dir /var/lib/rancher/rke2 )
      
      : Waiting for cluster to be ready
      kubectl wait --for=condition=Ready nodes --all --timeout=300s
      
      : Waiting for Cilium DaemonSet to be ready
      kubectl wait --for=condition=Ready pods -l k8s-app=cilium -n kube-system --timeout=300s
      
      : Waiting for Cilium operator to be ready
      kubectl wait --for=condition=Available deployment/cilium-operator -n kube-system --timeout=300s
      
      : Cilium Status Check
      cilium status --wait --wait-duration=300s
      
      : BGP Status Check (master node)
      cilium connectivity test --test-namespace cilium-test --single-node || true
      
      : LoadBalancer Pool Check
      kubectl get ciliumloadbalancerippool -o wide || true
      
      : Control Plane Service Check
      kubectl get svc control-plane-lb -n kube-system -o wide || true
      
      : Service Endpoints Check
      kubectl get endpoints control-plane-lb -n kube-system -o wide || true
      
      : VIP IP Pool Check
      kubectl get ciliumloadbalancerippool -o wide || true
      
      : BGP Configuration Check
      kubectl get ciliuml2announcementpolicy -o wide || true
      kubectl get ciliumbgpadvertisement -o wide || true
      kubectl get ciliumbgpclusterconfig -o wide || true
      kubectl get ciliumbgppeerconfig -o wide || true
      
      : API Connectivity Test
      if curl -k --connect-timeout 5 https://${CLUSTER_INET_VIRTUAL}:6443/version; then
        : ✅ API accessible via VIP ${CLUSTER_INET_VIRTUAL}
      else
        : ❌ API not accessible via VIP ${CLUSTER_INET_VIRTUAL}
      fi
      
      : Network Connectivity Summary
      cat <<EoSummary
      Node: ${CLUSTER_NODE_NAME}
      VIP: ${CLUSTER_INET_VIRTUAL}
      Node IP: ${CLUSTER_NODE_INET}
      Gateway: ${CLUSTER_GATEWAY}
      EoSummary

  # Master-only systemd override for post-start Cilium validation
  - path: /etc/systemd/system/rke2-server.service.d/post-start-master.conf
    content: |
      [Service]
      ExecStartPost=/bin/sh -xc '/usr/local/sbin/rke2-cilium-check'

  # =============================================================================
  # OTHER BOOTSTRAP MANIFESTS
  # =============================================================================

  # Traefik ingress controller configuration (bootstrap only)
  - path: /var/lib/rancher/rke2/server/manifests/rke2-traefik-config.yaml
    content: |
      apiVersion: helm.cattle.io/v1
      kind: HelmChartConfig
      metadata:
        name: rke2-traefik
        namespace: kube-system
      spec:
        valuesContent: |-
          additionalArguments:
            - "--api.insecure=true"
          ports:
            web:
              expose:
                default: true
            websecure:
              expose:
                default: true

  # Envoy Gateway installation (bootstrap only)
  - path: /var/lib/rancher/rke2/server/manifests/envoy-gateway.yaml
    content: |
      apiVersion: v1
      kind: Namespace
      metadata:
        name: envoy-gateway-system
      ---
      # Job to install Envoy Gateway via kubectl apply
      apiVersion: batch/v1
      kind: Job
      metadata:
        name: envoy-gateway-installer
        namespace: kube-system
      spec:
        template:
          spec:
            serviceAccountName: envoy-gateway-installer
            containers:
            - name: installer
              image: bitnami/kubectl:1.29.0
              command:
              - /bin/bash
              - -c
              - |
                kubectl apply --namespace envoy-gateway-system --server-side -f https://github.com/envoyproxy/gateway/releases/download/v1.4.2/install.yaml
            restartPolicy: OnFailure
        backoffLimit: 3
      ---
      # ServiceAccount for the installer job
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: envoy-gateway-installer
        namespace: envoy-gateway-system
      ---
      # ClusterRoleBinding for the installer
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        name: envoy-gateway-installer
      subjects:
      - kind: ServiceAccount
        name: envoy-gateway-installer
        namespace: envoy-gateway-system
      roleRef:
        kind: ClusterRole
        name: cluster-admin
        apiGroup: rbac.authorization.k8s.io
      ---
      apiVersion: gateway.networking.k8s.io/v1
      kind: GatewayClass
      metadata:
        name: envoy
      spec:
        controllerName: gateway.envoyproxy.io/gatewayclass-controller

  # OpenEBS ZFS storage provisioner (bootstrap only)
  - path: /var/lib/rancher/rke2/server/manifests/openebs-zfs.yaml
    content: |
      apiVersion: v1
      kind: Namespace
      metadata:
        name: openebs
      ---
      apiVersion: helm.cattle.io/v1
      kind: HelmChart
      metadata:
        name: openebs-zfs
        namespace: openebs
      spec:
        chart: zfs-localpv
        repo: https://openebs.github.io/zfs-localpv
        version: 2.8.0
        targetNamespace: openebs
        createNamespace: true
        valuesContent: |-
          zfsNode:
            kubeletDir: /var/lib/rancher/rke2/agent
      ---
      apiVersion: storage.k8s.io/v1
      kind: StorageClass
      metadata:
        name: openebs-zfs
      provisioner: zfs.csi.openebs.io
      parameters:
        poolname: tank
        fstype: zfs
      reclaimPolicy: Delete
      volumeBindingMode: WaitForFirstConsumer
      allowVolumeExpansion: true

  # Tailscale operator for VPN connectivity (bootstrap only)
  - path: /var/lib/rancher/rke2/server/manifests/tailscale-operator.yaml
    content: |
      apiVersion: helm.cattle.io/v1
      kind: HelmChart
      metadata:
        namespace: kube-system
        name: tailscale-operator
      spec:
        repo: https://pkgs.tailscale.com/helmcharts
        chart: tailscale-operator
        version: 1.82.0
        targetNamespace: tailscale-system
        createNamespace: true
        valuesContent: |-
          oauth:
            clientId: "${TSID}"
            clientSecret: "${TSKEY}"
          operatorConfig:
            hostname: "${CLUSTER_NAME}-tailscale-operator"
            debug: true
      ---
      apiVersion: tailscale.com/v1alpha1
      kind: Connector
      metadata:
        name: ts-controlplane-lb-routes
      spec:
        hostname: ${CLUSTER_NAME}-controlplane-lb-routes
        subnetRouter:
          advertiseRoutes:
